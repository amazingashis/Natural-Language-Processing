{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "Practical Assignment II Ashish Adhikari.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amazingashis/Natural-Language-Processing/blob/main/Practical_Assignment_II_Ashish_Adhikari.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNDAX5A_U6cC"
      },
      "source": [
        "# **Tokenization**\n",
        "Tokenization is one of the fundamental\n",
        "things to do in any text-processing activity. Tokenization can be thought of\n",
        "as a segmentation technique wherein you are trying to break down larger\n",
        "pieces of text chunks into smaller meaningful ones. Tokens generally\n",
        "comprise words and numbers, but they can be extended to include\n",
        "punctuation marks, symbols, and, at times, understandable emoticons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4fN6kCU6cG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82436606-f636-468b-fe4b-665b61a68016"
      },
      "source": [
        "sentence = \"The capital of China is Beijing\"\n",
        "sentence.split()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'capital', 'of', 'China', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfpTwtgfU6cI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb075296-896d-40b8-e704-89576f9494ec"
      },
      "source": [
        "sentence = \"China's capital is Beijing\"\n",
        "sentence.split()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"China's\", 'capital', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHhrKpkIU6cI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f494c2f3-ab49-4973-8f1f-23d5796f8091"
      },
      "source": [
        "sentence = \"Beijing is where we'll go\"\n",
        "sentence.split()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'where', \"we'll\", 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQGD3JCLU6cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df13610c-677f-4fa4-8bcd-f3cb5d132565"
      },
      "source": [
        "sentence = \"I'm going to travel to Beijing\"\n",
        "sentence.split()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'going', 'to', 'travel', 'to', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y79fI2vnU6cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa9c8c2-60ad-4026-fc2e-95eca1c46c59"
      },
      "source": [
        "sentence = \"A friend is pursuing his M.S from Beijing\"\n",
        "sentence.split()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tndanrhDU6cK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3fc4f9-6a87-45d3-8594-c7166c7ef7e0"
      },
      "source": [
        "sentence = \"Beijing is a cool place!!! :-P <3 #Awesome\"\n",
        "sentence.split()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi8f9gcSXOOv"
      },
      "source": [
        "# **Different types of tokenizers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT3YmLCbU6cL"
      },
      "source": [
        "### **Regular expressions-based tokenizers**\n",
        "The nltk package in Python provides a regular expressions-based\n",
        "tokenizers (RegexpTokenizer) functionality. It can be used to tokenize or split a sentence based on a provided regular expression. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtNCzROOU6cL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2539eb6-9b99-4bad-b351-33c85090b145"
      },
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(s)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcnJFJmfU6cN"
      },
      "source": [
        "### **Blankline Tokenizer**\n",
        "There are other tokenizers built on top of the RegexpTokenizer, such as the\n",
        "BlankLine tokenizer, which tokenizes a string treating blank lines as\n",
        "delimiters where blank lines are those that contain no characters except\n",
        "spaces or tabs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2H5eaZ6U6cO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dde38939-373a-442e-f5fd-082559d1c2ca"
      },
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n\\n I want a book as well\"\n",
        "tokenizer = BlanklineTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.',\n",
              " 'I want a book as well']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW2NSOAxU6cP"
      },
      "source": [
        "### **Tree Bank Tokenizer**\n",
        "The Treebank tokenizer also uses regular expressions to tokenize text\n",
        "according to the Penn Treebank.The Treebank tokenizer does a great job of splitting contractions such as\n",
        "doesn't to does and n't. It further identifies periods at the ends of lines and\n",
        "eliminates them. Punctuation such as commas is split if followed by\n",
        "whitespaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CI5qQjxU6cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed9c8a07-6b67-4eea-cde7-2c1ad5176d18"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"I'm going to buy a Rolex watch which doesn't cost more than $3000.0\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'which',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'cost',\n",
              " 'more',\n",
              " 'than',\n",
              " '$',\n",
              " '3000.0']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UQzsZjWU6cQ"
      },
      "source": [
        "###  **Tweet Tokenizer**\n",
        "the rise of social media has given rise to an informal\n",
        "language wherein people tag each other using their social media handles and\n",
        "use a lot of emoticons, hashtags, and abbreviated text to express themselves.\n",
        "We need tokenizers in place that can parse such text and make things more\n",
        "understandable. TweetTokenizer caters to this use case significantly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVVWUqPaU6cR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16fc6b3d-0e37-45d2-f7cd-4546f328d53b"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@amankedia',\n",
              " \"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaEA24JKU6cR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46bb8155-947b-48ac-c855-a99311577f77"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tokenizer.tokenize(s)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I76zNazFU6cS"
      },
      "source": [
        "# Understanding word Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWa5RmiNU6cS"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHwiEI42U6cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c3038b8-9b33-429d-f640-904cfb7fabfe"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "print(SnowballStemmer.languages)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmK8RwNxU6cT"
      },
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b2cU7HZU6cT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd9c4cf3-754b-4b55-df51-aa031b1cbe25"
      },
      "source": [
        "from nltk.stem.porter import PorterStemmer \n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtKS5ASxU6cU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59745b3d-a5d0-4695-afc7-a7c311484bd8"
      },
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "singles = [stemmer2.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1x2cNFwU6cU"
      },
      "source": [
        "## **Lemmatization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS6FRkppU6cU"
      },
      "source": [
        "#### **WordNet lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0vMUsMD3U6cV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f36488bf-f258-437c-c233-35b7fad4adf8"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USDEr_jLU6cV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d293d9a-e3b9-46da-c195-fe298b12a978"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "token_list = s.split()\n",
        "print(\"The tokens are: \", token_list)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
        "print(\"The lemmatized output is: \", lemmatized_output)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sETOMbR6U6cV"
      },
      "source": [
        "#### **POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L59_LGAEU6cW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ae5653-4573-4f61-c0b1-1b25b8a0224a"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(token_list)\n",
        "pos_tags"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('putting', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('efforts', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enhance', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('understanding', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Lemmatization', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2BobLhSU6cW"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
        "\n",
        "def get_part_of_speech_tags(token):\n",
        "    \n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    \n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "    \n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_1vJwvIU6cX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860f2852-40a5-429e-f872-89901f1ecf25"
      },
      "source": [
        "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
        "print(' '.join(lemmatized_output_with_POS_information))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We be put in effort to enhance our understand of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaSFtIIgU6cX"
      },
      "source": [
        "####  **Comparision**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UceuWOWfU6cY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0b6607f-dc86-4e91-9e46-1f702930d761"
      },
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
        "print(' '.join(stemmed_sentence))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are put in effort to enhanc our understand of lemmat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udzLUipCU6cY"
      },
      "source": [
        "#### **Spacy lemmatizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2I0U2mSU6cY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7fb497c7-957a-4764-8724-4e21c2527398"
      },
      "source": [
        "import spacy\n",
        "nlp =spacy.load('en_core_web_sm')\n",
        "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
        "\" \".join([token.lemma_ for token in doc])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'-PRON- be put in effort to enhance -PRON- understanding of lemmatization'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paNgNgr2U6cY"
      },
      "source": [
        "#### **StopsWord Removal**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHVKpDGqU6cZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "041b88aa-8470-4554-ba11-9037f26a0dd2"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\", \".join(stop)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"her, should've, is, after, ma, during, again, don, will, himself, same, don't, didn, which, he, up, wouldn't, other, myself, s, t, o, ve, i, most, yours, we, had, any, won, few, mustn't, hers, him, herself, this, very, on, between, itself, doesn, ourselves, who, has, wouldn, hasn't, hadn, doesn't, through, my, haven, re, hadn't, because, aren, more, there, than, can, needn't, couldn, ours, in, am, it, ain, that, weren't, couldn't, off, shan, needn, being, be, for, not, that'll, them, but, she's, it's, while, only, such, to, below, of, down, did, those, their, both, ll, you'll, its, by, aren't, no, does, d, doing, over, from, so, or, just, been, she, your, under, our, you're, a, won't, shouldn't, as, you've, isn, too, theirs, how, at, until, mightn, and, before, nor, shouldn, whom, wasn, were, mustn, above, you, why, all, shan't, yourselves, once, me, some, yourself, own, was, haven't, with, wasn't, having, should, you'd, if, then, didn't, do, the, here, now, what, his, weren, they, y, these, are, an, when, isn't, hasn, where, about, each, m, mightn't, themselves, further, into, out, have, against\""
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCF7wXpPU6cZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0a63fa54-b185-413e-e277-a57bfd9cc012"
      },
      "source": [
        "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "for word in wh_words:\n",
        "    stop.remove(word)\n",
        "\n",
        "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
        "\" \".join(sentence_after_stopword_removal)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'how putting efforts enhance understanding Lemmatization'"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdSc2BMdU6cZ"
      },
      "source": [
        "#### **Case Folding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO3m4Jy4U6ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ced4015-5162-4c73-c13f-947ec94b73a3"
      },
      "source": [
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "s = s.lower()\n",
        "s"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'we are putting in efforts to enhance our understanding of lemmatization'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGdN_jNmU6ca"
      },
      "source": [
        "### **N-Grams**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_H7zKMSU6ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a68ee025-f9e2-473d-e8c5-38c4cdd53bb7"
      },
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "[\" \".join(token) for token in bigrams]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language',\n",
              " 'Language Processing',\n",
              " 'Processing is',\n",
              " 'is the',\n",
              " 'the way',\n",
              " 'way to',\n",
              " 'to go']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS1DDupzU6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "872615e1-b89a-48fd-d223-58e3f0c00226"
      },
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "[\" \".join(token) for token in trigrams]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing',\n",
              " 'Language Processing is',\n",
              " 'Processing is the',\n",
              " 'is the way',\n",
              " 'the way to',\n",
              " 'way to go']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOuPB4vuU6cb"
      },
      "source": [
        "### **Removing HTMl Tags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCF_e4bJU6cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2f1a35a-088d-4156-b26b-5e0ba64fa465"
      },
      "source": [
        "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html)\n",
        "text = soup.get_text()\n",
        "print(text)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My First HeadingMy first paragraph.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIdWgBStU6cb"
      },
      "source": [
        ""
      ],
      "execution_count": 28,
      "outputs": []
    }
  ]
}